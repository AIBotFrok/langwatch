{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM test response: ['Hello! How can I assist you today?']\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "\n",
    "llm = dspy.OpenAI(\n",
    "    model_type=\"chat\",\n",
    "    # model=\"groq/llama3-70b-8192\",\n",
    "    # model=\"azure/gpt-35-turbo-1106\",\n",
    "    model=\"openai/gpt-3.5-turbo\",\n",
    "    # model=\"gpt-3.5-turbo\",\n",
    "    # model=\"anthropic/claude-3-haiku-20240307\",\n",
    "    api_base=\"http://localhost:8080/proxy/v1/\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"LLM test response:\", llm(\"hello there\"))\n",
    "\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "dspy.settings.configure(lm=llm, rm=colbertv2_wiki17_abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 50)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.datasets import HotPotQA\n",
    "\n",
    "# Load the dataset.\n",
    "dataset = HotPotQA(train_seed=1, train_size=32, eval_seed=2023, dev_size=50, test_size=0)\n",
    "\n",
    "# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\n",
    "trainset = [x.with_inputs('question') for x in dataset.train]\n",
    "devset = [x.with_inputs('question') for x in dataset.dev]\n",
    "\n",
    "len(trainset), len(devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Devset] Question: What is the nationality of the chef and restaurateur featured in Restaurant: Impossible?\n",
      "[Devset] Answer: English\n",
      "[Devset] Relevant Wikipedia Titles: {'Restaurant: Impossible', 'Robert Irvine'}\n",
      "[Prediction] Question: What is the nationality of the chef and restaurateur featured in Restaurant: Impossible?\n",
      "[Prediction] Predicted Answer: British\n"
     ]
    }
   ],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
    "\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)\n",
    "\n",
    "\n",
    "dev_example = devset[18]\n",
    "print(f\"[Devset] Question: {dev_example.question}\")\n",
    "print(f\"[Devset] Answer: {dev_example.answer}\")\n",
    "print(f\"[Devset] Relevant Wikipedia Titles: {dev_example.gold_titles}\")\n",
    "\n",
    "generate_answer = RAG()\n",
    "\n",
    "pred = generate_answer(question=dev_example.question)\n",
    "\n",
    "# Print the input and the prediction.\n",
    "print(f\"[Prediction] Question: {dev_example.question}\")\n",
    "print(f\"[Prediction] Predicted Answer: {pred.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to LangWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangWatch API key is already set, if you want to login again, please call as langwatch.login(relogin=True)\n"
     ]
    }
   ],
   "source": [
    "import langwatch\n",
    "\n",
    "langwatch.endpoint = \"http://localhost:3000\"\n",
    "langwatch.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refetching langwatch modules for development\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"langwatch\" in sys.modules:\n",
    "    del sys.modules[\"langwatch\"]\n",
    "if \"langwatch.dspy\" in sys.modules:\n",
    "    del sys.modules[\"langwatch.dspy\"]\n",
    "\n",
    "import langwatch\n",
    "from langwatch.dspy import SerializableAndPydanticEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training Session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment initialized, run_id: great-weasel-of-action\n",
      "Open http://localhost:3000/inbox-narrator/experiments/dspy-visualizer-example?runIds=great-weasel-of-action to track your DSPy training session live\n",
      "\n",
      "Iteration Depth: 1/3.\n",
      "At Depth 1/3, Evaluating Prompt Candidate #1/2 for Predictor 1 of 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9 / 32  (28.1): 100%|██████████| 32/32 [00:00<00:00, 1205.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9 / 32 (28.1%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Depth 1/3, Evaluating Prompt Candidate #2/2 for Predictor 1 of 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 32  (34.4): 100%|██████████| 32/32 [00:00<00:00, 1176.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 32 (34.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Depth: 2/3.\n",
      "At Depth 2/3, Evaluating Prompt Candidate #1/2 for Predictor 1 of 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6 / 32  (18.8): 100%|██████████| 32/32 [00:00<00:00, 1002.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6 / 32 (18.8%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Depth 2/3, Evaluating Prompt Candidate #2/2 for Predictor 1 of 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4 / 32  (12.5): 100%|██████████| 32/32 [00:00<00:00, 1120.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4 / 32 (12.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Depth: 3/3.\n",
      "At Depth 3/3, Evaluating Prompt Candidate #1/2 for Predictor 1 of 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6 / 32  (18.8): 100%|██████████| 32/32 [00:00<00:00, 1555.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6 / 32 (18.8%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Depth 3/3, Evaluating Prompt Candidate #2/2 for Predictor 1 of 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4 / 32  (12.5): 100%|██████████| 32/32 [00:00<00:00, 782.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4 / 32 (12.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, LabeledFewShot, BootstrapFewShot, COPRO\n",
    "import dspy.teleprompt\n",
    "import dspy.evaluate\n",
    "\n",
    "# make logger appear on jupyter notebook\n",
    "dspy.logger.info = print\n",
    "\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
    "    return answer_EM and answer_PM\n",
    "\n",
    "# Set up a basic optimizer, which will compile our RAG program.\n",
    "# optimizer = BootstrapFewShotWithRandomSearch(metric=validate_context_and_answer, max_rounds=1)\n",
    "# optimizer = BootstrapFewShot(metric=validate_context_and_answer, max_bootstrapped_demos=10, max_labeled_demos=10, max_rounds=3)\n",
    "optimizer = COPRO(metric=validate_context_and_answer, breadth=2, depth=3, track_stats=True)\n",
    "# optimizer.num_candidate_sets = 0\n",
    "\n",
    "langwatch.dspy.init(experiment=\"dspy-visualizer-example\", optimizer=optimizer)\n",
    "\n",
    "# Compile\n",
    "compiled_rag = optimizer.compile(RAG(), trainset=trainset, eval_kwargs=dict(num_threads=64, display_progress=True, display_table=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rag.save(\"test.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
