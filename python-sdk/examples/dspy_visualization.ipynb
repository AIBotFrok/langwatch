{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM test response: ['Hello! How can I assist you today?']\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "\n",
    "llm = dspy.OpenAI(\n",
    "    model_type=\"chat\",\n",
    "    # model=\"groq/llama3-70b-8192\",\n",
    "    # model=\"azure/gpt-35-turbo-1106\",\n",
    "    model=\"openai/gpt-3.5-turbo\",\n",
    "    # model=\"gpt-3.5-turbo\",\n",
    "    # model=\"anthropic/claude-3-haiku-20240307\",\n",
    "    api_base=\"http://localhost:8080/proxy/v1/\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"LLM test response:\", llm(\"hello there\"))\n",
    "\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "dspy.settings.configure(lm=llm, rm=colbertv2_wiki17_abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 50)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.datasets import HotPotQA\n",
    "\n",
    "# Load the dataset.\n",
    "dataset = HotPotQA(train_seed=1, train_size=32, eval_seed=2023, dev_size=50, test_size=0)\n",
    "\n",
    "# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\n",
    "trainset = [x.with_inputs('question') for x in dataset.train]\n",
    "devset = [x.with_inputs('question') for x in dataset.dev]\n",
    "\n",
    "len(trainset), len(devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Devset] Question: What is the nationality of the chef and restaurateur featured in Restaurant: Impossible?\n",
      "[Devset] Answer: English\n",
      "[Devset] Relevant Wikipedia Titles: {'Robert Irvine', 'Restaurant: Impossible'}\n",
      "[Prediction] Question: What is the nationality of the chef and restaurateur featured in Restaurant: Impossible?\n",
      "[Prediction] Predicted Answer: British\n"
     ]
    }
   ],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
    "\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)\n",
    "\n",
    "\n",
    "dev_example = devset[18]\n",
    "print(f\"[Devset] Question: {dev_example.question}\")\n",
    "print(f\"[Devset] Answer: {dev_example.answer}\")\n",
    "print(f\"[Devset] Relevant Wikipedia Titles: {dev_example.gold_titles}\")\n",
    "\n",
    "generate_answer = RAG()\n",
    "\n",
    "pred = generate_answer(question=dev_example.question)\n",
    "\n",
    "# Print the input and the prediction.\n",
    "print(f\"[Prediction] Question: {dev_example.question}\")\n",
    "print(f\"[Prediction] Predicted Answer: {pred.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to LangWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangWatch API key is already set, if you want to login again, please call as langwatch.login(relogin=True)\n"
     ]
    }
   ],
   "source": [
    "import langwatch\n",
    "\n",
    "langwatch.endpoint = \"http://localhost:3000\"\n",
    "langwatch.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refetching langwatch modules for development\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"langwatch\" in sys.modules:\n",
    "    del sys.modules[\"langwatch\"]\n",
    "if \"langwatch.dspy\" in sys.modules:\n",
    "    del sys.modules[\"langwatch.dspy\"]\n",
    "\n",
    "import langwatch\n",
    "from langwatch.dspy import SerializableAndPydanticEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training Session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment initialized, run_id: convivial-enchanted-catfish\n",
      "Open http://localhost:3000/inbox-narrator/experiments/dspy-visualizer-example?runIds=convivial-enchanted-catfish to track your DSPy training session live\n",
      "\n",
      "\u001b[93m\u001b[1mWARNING: Projected Language Model (LM) Calls\u001b[0m\n",
      "\n",
      "Please be advised that based on the parameters you have set, the maximum number of LM calls is projected as follows:\n",
      "\n",
      "\u001b[93m- Task Model: \u001b[94m\u001b[1m32\u001b[0m\u001b[93m examples in dev set * \u001b[94m\u001b[1m10\u001b[0m\u001b[93m trials * \u001b[94m\u001b[1m# of LM calls in your program\u001b[0m\u001b[93m = (\u001b[94m\u001b[1m320 * # of LM calls in your program\u001b[0m\u001b[93m) task model calls\u001b[0m\n",
      "\u001b[93m- Prompt Model: # data summarizer calls (max \u001b[94m\u001b[1m10\u001b[0m\u001b[93m) + \u001b[94m\u001b[1m2\u001b[0m\u001b[93m * \u001b[94m\u001b[1m1\u001b[0m\u001b[93m lm calls in program = \u001b[94m\u001b[1m12\u001b[0m\u001b[93m prompt model calls\u001b[0m\n",
      "\n",
      "\u001b[93m\u001b[1mEstimated Cost Calculation:\u001b[0m\n",
      "\n",
      "\u001b[93mTotal Cost = (Number of calls to task model * (Avg Input Token Length per Call * Task Model Price per Input Token + Avg Output Token Length per Call * Task Model Price per Output Token) \n",
      "            + (Number of calls to prompt model * (Avg Input Token Length per Call * Task Prompt Price per Input Token + Avg Output Token Length per Call * Prompt Model Price per Output Token).\u001b[0m\n",
      "\n",
      "For a preliminary estimate of potential costs, we recommend you perform your own calculations based on the task\n",
      "and prompt models you intend to use. If the projected costs exceed your budget or expectations, you may consider:\n",
      "\n",
      "\u001b[93m- Reducing the number of trials (`num_trials`), the size of the trainset, or the number of LM calls in your program.\u001b[0m\n",
      "\u001b[93m- Using a cheaper task model to optimize the prompt.\u001b[0m\n",
      "To proceed with the execution of this program, please confirm by typing \u001b[94m'y'\u001b[0m for yes or \u001b[94m'n'\u001b[0m for no.\n",
      "\n",
      "If you would like to bypass this confirmation step in future executions, set the \u001b[93m`requires_permission_to_run`\u001b[0m flag to \u001b[93m`False`.\u001b[0m\n",
      "\n",
      "\u001b[93mAwaiting your input...\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 4/32 [00:00<00:00, 907.61it/s]\n",
      "[I 2024-05-29 20:10:22,198] A new study created in memory with name: no-name-15828445-319a-471b-a5be-9986b42b0db0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n",
      "Starting trial #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8 / 32  (25.0): 100%|██████████| 32/32 [00:00<00:00, 590.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8 / 32 (25.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-29 20:10:22,422] Trial 0 finished with value: 25.0 and parameters: {'6056878432_predictor_instruction': 1, '6056878432_predictor_demos': 0}. Best is trial 0 with value: 25.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 32  (31.2): 100%|██████████| 32/32 [00:00<00:00, 677.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 32 (31.2%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-29 20:10:22,635] Trial 1 finished with value: 31.25 and parameters: {'6056878432_predictor_instruction': 0, '6056878432_predictor_demos': 1}. Best is trial 1 with value: 31.25.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32  (40.6): 100%|██████████| 32/32 [00:00<00:00, 626.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32 (40.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-29 20:10:22,843] Trial 2 finished with value: 40.62 and parameters: {'6056878432_predictor_instruction': 1, '6056878432_predictor_demos': 1}. Best is trial 2 with value: 40.62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 32  (31.2): 100%|██████████| 32/32 [00:00<00:00, 666.47it/s]\n",
      "[I 2024-05-29 20:10:23,047] Trial 3 finished with value: 31.25 and parameters: {'6056878432_predictor_instruction': 0, '6056878432_predictor_demos': 1}. Best is trial 2 with value: 40.62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 32 (31.2%)\n",
      "Starting trial #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8 / 32  (25.0): 100%|██████████| 32/32 [00:00<00:00, 596.53it/s]\n",
      "[I 2024-05-29 20:10:23,252] Trial 4 finished with value: 25.0 and parameters: {'6056878432_predictor_instruction': 1, '6056878432_predictor_demos': 0}. Best is trial 2 with value: 40.62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8 / 32 (25.0%)\n",
      "Starting trial #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 32  (31.2): 100%|██████████| 32/32 [00:00<00:00, 597.35it/s]\n",
      "[I 2024-05-29 20:10:23,467] Trial 5 finished with value: 31.25 and parameters: {'6056878432_predictor_instruction': 0, '6056878432_predictor_demos': 1}. Best is trial 2 with value: 40.62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 32 (31.2%)\n",
      "Starting trial #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32  (40.6): 100%|██████████| 32/32 [00:00<00:00, 648.99it/s]\n",
      "[I 2024-05-29 20:10:23,668] Trial 6 finished with value: 40.62 and parameters: {'6056878432_predictor_instruction': 1, '6056878432_predictor_demos': 1}. Best is trial 2 with value: 40.62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32 (40.6%)\n",
      "Starting trial #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 32  (34.4): 100%|██████████| 32/32 [00:00<00:00, 689.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 32 (34.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-29 20:10:23,871] Trial 7 finished with value: 34.38 and parameters: {'6056878432_predictor_instruction': 0, '6056878432_predictor_demos': 0}. Best is trial 2 with value: 40.62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8 / 32  (25.0): 100%|██████████| 32/32 [00:00<00:00, 764.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8 / 32 (25.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-29 20:10:24,088] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial pruned.\n",
      "Starting trial #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 32  (34.4): 100%|██████████| 32/32 [00:00<00:00, 634.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 32 (34.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2024-05-29 20:10:24,313] Trial 9 finished with value: 34.38 and parameters: {'6056878432_predictor_instruction': 0, '6056878432_predictor_demos': 0}. Best is trial 2 with value: 40.62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning generate_answer = ChainOfThought(GenerateAnswer(context, question -> answer\n",
      "    instructions='Answer questions with short factoid answers.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'desc': 'may contain relevant facts', '__dspy_field_type': 'input', 'prefix': 'Context:'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'often between 1 and 5 words', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")) from continue_program\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import (\n",
    "    BootstrapFewShotWithRandomSearch,\n",
    "    LabeledFewShot,\n",
    "    BootstrapFewShot,\n",
    "    COPRO,\n",
    "    MIPRO,\n",
    ")\n",
    "import dspy.teleprompt\n",
    "import dspy.evaluate\n",
    "\n",
    "# make logger appear on jupyter notebook\n",
    "dspy.logger.info = print\n",
    "\n",
    "\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
    "    return answer_EM and answer_PM\n",
    "\n",
    "\n",
    "# Set up a basic optimizer, which will compile our RAG program.\n",
    "# optimizer = BootstrapFewShotWithRandomSearch(metric=validate_context_and_answer, max_rounds=1)\n",
    "# optimizer = BootstrapFewShot(metric=validate_context_and_answer, max_bootstrapped_demos=10, max_labeled_demos=10, max_rounds=3)\n",
    "# optimizer = COPRO(metric=validate_context_and_answer, breadth=2, depth=3, track_stats=True)\n",
    "optimizer = MIPRO(\n",
    "    metric=validate_context_and_answer,\n",
    "    num_candidates=2,\n",
    "    init_temperature=0.7\n",
    ")\n",
    "# optimizer.num_candidate_sets = 0\n",
    "\n",
    "langwatch.dspy.init(experiment=\"dspy-visualizer-example\", optimizer=optimizer)\n",
    "\n",
    "# Compile\n",
    "# compiled_rag = optimizer.compile(RAG(), trainset=trainset, eval_kwargs=dict(num_threads=64, display_progress=True, display_table=0))\n",
    "compiled_rag = optimizer.compile(\n",
    "    RAG(),\n",
    "    trainset=trainset,\n",
    "    num_trials=10,\n",
    "    max_bootstrapped_demos=3,\n",
    "    max_labeled_demos=5,\n",
    "    eval_kwargs=dict(num_threads=16, display_progress=True, display_table=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rag.save(\"test.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
