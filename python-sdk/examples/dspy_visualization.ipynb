{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM test response: ['Hello! How can I assist you today?']\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "\n",
    "llm = dspy.OpenAI(\n",
    "    model_type=\"chat\",\n",
    "    # model=\"groq/llama3-70b-8192\",\n",
    "    # model=\"azure/gpt-35-turbo-1106\",\n",
    "    # model=\"openai/gpt-3.5-turbo\",\n",
    "    # model=\"gpt-3.5-turbo\",\n",
    "    model=\"anthropic/claude-3-haiku-20240307\",\n",
    "    api_base=\"http://localhost:8080/proxy/v1/\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"LLM test response:\", llm(\"hello there\"))\n",
    "\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "dspy.settings.configure(lm=llm, rm=colbertv2_wiki17_abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 50)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.datasets import HotPotQA\n",
    "\n",
    "# Load the dataset.\n",
    "dataset = HotPotQA(train_seed=1, train_size=32, eval_seed=2023, dev_size=50, test_size=0)\n",
    "\n",
    "# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\n",
    "trainset = [x.with_inputs('question') for x in dataset.train]\n",
    "devset = [x.with_inputs('question') for x in dataset.dev]\n",
    "\n",
    "len(trainset), len(devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Devset] Question: What is the nationality of the chef and restaurateur featured in Restaurant: Impossible?\n",
      "[Devset] Answer: English\n",
      "[Devset] Relevant Wikipedia Titles: {'Restaurant: Impossible', 'Robert Irvine'}\n",
      "[Prediction] Question: What is the nationality of the chef and restaurateur featured in Restaurant: Impossible?\n",
      "[Prediction] Predicted Answer: British\n"
     ]
    }
   ],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
    "\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)\n",
    "\n",
    "\n",
    "dev_example = devset[18]\n",
    "print(f\"[Devset] Question: {dev_example.question}\")\n",
    "print(f\"[Devset] Answer: {dev_example.answer}\")\n",
    "print(f\"[Devset] Relevant Wikipedia Titles: {dev_example.gold_titles}\")\n",
    "\n",
    "generate_answer = RAG()\n",
    "\n",
    "pred = generate_answer(question=dev_example.question)\n",
    "\n",
    "# Print the input and the prediction.\n",
    "print(f\"[Prediction] Question: {dev_example.question}\")\n",
    "print(f\"[Prediction] Predicted Answer: {pred.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to LangWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangWatch API key is already set, if you want to login again, please call as langwatch.login(relogin=True)\n"
     ]
    }
   ],
   "source": [
    "import langwatch\n",
    "\n",
    "langwatch.endpoint = \"http://localhost:3000\"\n",
    "langwatch.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refetching langwatch modules for development\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"langwatch\" in sys.modules:\n",
    "    del sys.modules[\"langwatch\"]\n",
    "if \"langwatch.dspy\" in sys.modules:\n",
    "    del sys.modules[\"langwatch.dspy\"]\n",
    "\n",
    "import langwatch\n",
    "from langwatch.dspy import SerializableAndPydanticEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training Session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 4 traces per predictor.\n",
      "Will attempt to train 16 candidate sets.\n",
      "Experiment initialized, open http://localhost:3000/inbox-narrator/experiments/dspy-visualizer-example to track your DSPy training session live\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9 / 32  (28.1): 100%|██████████| 32/32 [00:02<00:00, 14.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9 / 32 (28.1%)\n",
      "Score: 28.12 for set: [0]\n",
      "New best sscore: 28.12 for seed -3\n",
      "Scores so far: [28.12]\n",
      "Best score: 28.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12 / 32  (37.5): 100%|██████████| 32/32 [00:11<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12 / 32 (37.5%)\n",
      "Score: 37.5 for set: [16]\n",
      "New best sscore: 37.5 for seed -2\n",
      "Scores so far: [28.12, 37.5]\n",
      "Best score: 37.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 11/32 [00:09<00:18,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 12 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12 / 32  (37.5): 100%|██████████| 32/32 [00:09<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12 / 32 (37.5%)\n",
      "Score: 37.5 for set: [16]\n",
      "Scores so far: [28.12, 37.5, 37.5]\n",
      "Best score: 37.5\n",
      "Average of max per entry across top 1 scores: 0.375\n",
      "Average of max per entry across top 2 scores: 0.5\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 13/32 [00:25<00:37,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 14 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32  (40.6): 100%|██████████| 32/32 [00:10<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32 (40.6%)\n",
      "Score: 40.62 for set: [16]\n",
      "New best sscore: 40.62 for seed 0\n",
      "Scores so far: [28.12, 37.5, 37.5, 40.62]\n",
      "Best score: 40.62\n",
      "Average of max per entry across top 1 scores: 0.40625\n",
      "Average of max per entry across top 2 scores: 0.46875\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14 / 32  (43.8): 100%|██████████| 32/32 [00:09<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14 / 32 (43.8%)\n",
      "Score: 43.75 for set: [16]\n",
      "New best sscore: 43.75 for seed 1\n",
      "Scores so far: [28.12, 37.5, 37.5, 40.62, 43.75]\n",
      "Best score: 43.75\n",
      "Average of max per entry across top 1 scores: 0.4375\n",
      "Average of max per entry across top 2 scores: 0.46875\n",
      "Average of max per entry across top 3 scores: 0.46875\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 4/32 [00:07<00:51,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32  (40.6): 100%|██████████| 32/32 [00:11<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32 (40.6%)\n",
      "Score: 40.62 for set: [16]\n",
      "Scores so far: [28.12, 37.5, 37.5, 40.62, 43.75, 40.62]\n",
      "Best score: 43.75\n",
      "Average of max per entry across top 1 scores: 0.4375\n",
      "Average of max per entry across top 2 scores: 0.46875\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 4/32 [00:09<01:07,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 32  (31.2): 100%|██████████| 32/32 [00:10<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 32 (31.2%)\n",
      "Score: 31.25 for set: [16]\n",
      "Scores so far: [28.12, 37.5, 37.5, 40.62, 43.75, 40.62, 31.25]\n",
      "Best score: 43.75\n",
      "Average of max per entry across top 1 scores: 0.4375\n",
      "Average of max per entry across top 2 scores: 0.46875\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 8/32 [00:15<00:47,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32  (40.6): 100%|██████████| 32/32 [00:10<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32 (40.6%)\n",
      "Score: 40.62 for set: [16]\n",
      "Scores so far: [28.12, 37.5, 37.5, 40.62, 43.75, 40.62, 31.25, 40.62]\n",
      "Best score: 43.75\n",
      "Average of max per entry across top 1 scores: 0.4375\n",
      "Average of max per entry across top 2 scores: 0.46875\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 6/32 [00:12<00:53,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12 / 32  (37.5): 100%|██████████| 32/32 [00:10<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12 / 32 (37.5%)\n",
      "Score: 37.5 for set: [16]\n",
      "Scores so far: [28.12, 37.5, 37.5, 40.62, 43.75, 40.62, 31.25, 40.62, 37.5]\n",
      "Best score: 43.75\n",
      "Average of max per entry across top 1 scores: 0.4375\n",
      "Average of max per entry across top 2 scores: 0.46875\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 4/32 [00:07<00:51,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32  (40.6): 100%|██████████| 32/32 [00:09<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32 (40.6%)\n",
      "Score: 40.62 for set: [16]\n",
      "Scores so far: [28.12, 37.5, 37.5, 40.62, 43.75, 40.62, 31.25, 40.62, 37.5, 40.62]\n",
      "Best score: 43.75\n",
      "Average of max per entry across top 1 scores: 0.4375\n",
      "Average of max per entry across top 2 scores: 0.46875\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 7/32 [00:14<00:50,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 32  (34.4): 100%|██████████| 32/32 [00:10<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 32 (34.4%)\n",
      "Score: 34.38 for set: [16]\n",
      "Scores so far: [28.12, 37.5, 37.5, 40.62, 43.75, 40.62, 31.25, 40.62, 37.5, 40.62, 34.38]\n",
      "Best score: 43.75\n",
      "Average of max per entry across top 1 scores: 0.4375\n",
      "Average of max per entry across top 2 scores: 0.46875\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 9/32 [00:15<00:39,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32  (40.6): 100%|██████████| 32/32 [00:09<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32 (40.6%)\n",
      "Score: 40.62 for set: [16]\n",
      "Scores so far: [28.12, 37.5, 37.5, 40.62, 43.75, 40.62, 31.25, 40.62, 37.5, 40.62, 34.38, 40.62]\n",
      "Best score: 43.75\n",
      "Average of max per entry across top 1 scores: 0.4375\n",
      "Average of max per entry across top 2 scores: 0.46875\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 13/32 [00:24<00:36,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 14 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32  (40.6): 100%|██████████| 32/32 [00:10<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 32 (40.6%)\n",
      "Score: 40.62 for set: [16]\n",
      "Scores so far: [28.12, 37.5, 37.5, 40.62, 43.75, 40.62, 31.25, 40.62, 37.5, 40.62, 34.38, 40.62, 40.62]\n",
      "Best score: 43.75\n",
      "Average of max per entry across top 1 scores: 0.4375\n",
      "Average of max per entry across top 2 scores: 0.46875\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/32 [00:01<00:52,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 32  (34.4): 100%|██████████| 32/32 [00:10<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 32 (34.4%)\n",
      "Score: 34.38 for set: [16]\n",
      "Scores so far: [28.12, 37.5, 37.5, 40.62, 43.75, 40.62, 31.25, 40.62, 37.5, 40.62, 34.38, 40.62, 40.62, 34.38]\n",
      "Best score: 43.75\n",
      "Average of max per entry across top 1 scores: 0.4375\n",
      "Average of max per entry across top 2 scores: 0.46875\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 6/32 [00:11<00:48,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8 / 32  (25.0): 100%|██████████| 32/32 [00:10<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8 / 32 (25.0%)\n",
      "Score: 25.0 for set: [16]\n",
      "Scores so far: [28.12, 37.5, 37.5, 40.62, 43.75, 40.62, 31.25, 40.62, 37.5, 40.62, 34.38, 40.62, 40.62, 34.38, 25.0]\n",
      "Best score: 43.75\n",
      "Average of max per entry across top 1 scores: 0.4375\n",
      "Average of max per entry across top 2 scores: 0.46875\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 9/32 [00:16<00:41,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16 / 32  (50.0): 100%|██████████| 32/32 [00:09<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16 / 32 (50.0%)\n",
      "Score: 50.0 for set: [16]\n",
      "New best sscore: 50.0 for seed 12\n",
      "Scores so far: [28.12, 37.5, 37.5, 40.62, 43.75, 40.62, 31.25, 40.62, 37.5, 40.62, 34.38, 40.62, 40.62, 34.38, 25.0, 50.0]\n",
      "Best score: 50.0\n",
      "Average of max per entry across top 1 scores: 0.5\n",
      "Average of max per entry across top 2 scores: 0.53125\n",
      "Average of max per entry across top 3 scores: 0.53125\n",
      "Average of max per entry across top 5 scores: 0.53125\n",
      "Average of max per entry across top 8 scores: 0.53125\n",
      "Average of max per entry across top 9999 scores: 0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 12/32 [00:22<00:36,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 13 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 32  (31.2): 100%|██████████| 32/32 [00:09<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 32 (31.2%)\n",
      "Score: 31.25 for set: [16]\n",
      "Scores so far: [28.12, 37.5, 37.5, 40.62, 43.75, 40.62, 31.25, 40.62, 37.5, 40.62, 34.38, 40.62, 40.62, 34.38, 25.0, 50.0, 31.25]\n",
      "Best score: 50.0\n",
      "Average of max per entry across top 1 scores: 0.5\n",
      "Average of max per entry across top 2 scores: 0.53125\n",
      "Average of max per entry across top 3 scores: 0.53125\n",
      "Average of max per entry across top 5 scores: 0.53125\n",
      "Average of max per entry across top 8 scores: 0.53125\n",
      "Average of max per entry across top 9999 scores: 0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/32 [00:02<01:18,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9 / 24  (37.5):  72%|███████▏  | 23/32 [00:07<00:02,  3.20it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m langwatch\u001b[38;5;241m.\u001b[39mdspy\u001b[38;5;241m.\u001b[39minit(experiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdspy-visualizer-example\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Compile!\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m compiled_rag \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRAG\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/langwatch-saas/langwatch/python-sdk/langwatch/dspy.py:316\u001b[0m, in \u001b[0;36mLangWatchTrackedBootstrapFewShotWithRandomSearch.compile\u001b[0;34m(self, student, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, student, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_patch_evaluate():\n\u001b[0;32m--> 316\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.9/site-packages/dspy/teleprompt/random_search.py:124\u001b[0m, in \u001b[0;36mBootstrapFewShotWithRandomSearch.compile\u001b[0;34m(self, student, teacher, trainset, valset, restrict, labeled_sample)\u001b[0m\n\u001b[1;32m    113\u001b[0m     program2 \u001b[38;5;241m=\u001b[39m teleprompter\u001b[38;5;241m.\u001b[39mcompile(student, teacher\u001b[38;5;241m=\u001b[39mteacher, trainset\u001b[38;5;241m=\u001b[39mtrainset2)\n\u001b[1;32m    115\u001b[0m evaluate \u001b[38;5;241m=\u001b[39m Evaluate(\n\u001b[1;32m    116\u001b[0m     devset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalset,\n\u001b[1;32m    117\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     display_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    122\u001b[0m )\n\u001b[0;32m--> 124\u001b[0m score, subscores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogram2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m all_subscores\u001b[38;5;241m.\u001b[39mappend(subscores)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m############ Assertion-aware Optimization ############\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/langwatch-saas/langwatch/python-sdk/langwatch/dspy.py:332\u001b[0m, in \u001b[0;36mLangWatchTrackedBootstrapFewShotWithRandomSearch._patch_evaluate.<locals>.patched_evaluate_call\u001b[0;34m(self, program, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_all_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_all_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_all_scores is not True for some reason, please report it at https://github.com/langwatch/langwatch/issues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m     )\n\u001b[0;32m--> 332\u001b[0m score, subscores \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_evaluate_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    334\u001b[0m langwatch_dspy\u001b[38;5;241m.\u001b[39mlog_step(\n\u001b[1;32m    335\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mDSPyOptimizer(\n\u001b[1;32m    336\u001b[0m         name\u001b[38;5;241m=\u001b[39mBootstrapFewShotWithRandomSearch\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m     ],\n\u001b[1;32m    355\u001b[0m )\n\u001b[1;32m    357\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.9/site-packages/dspy/evaluate/evaluate.py:160\u001b[0m, in \u001b[0;36mEvaluate.__call__\u001b[0;34m(self, program, metric, devset, num_threads, display_progress, display_table, return_all_scores, return_outputs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_single_thread(wrapped_program, devset, display_progress)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_multi_thread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapped_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m dspy\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Metric: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mncorrect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mntotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mncorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mntotal,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m predicted_devset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(reordered_devset)\n",
      "File \u001b[0;32m~/Projects/langwatch-saas/langwatch/python-sdk/.venv/lib/python3.9/site-packages/dspy/evaluate/evaluate.py:86\u001b[0m, in \u001b[0;36mEvaluate._execute_multi_thread\u001b[0;34m(self, wrapped_program, devset, num_threads, display_progress)\u001b[0m\n\u001b[1;32m     83\u001b[0m futures \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(wrapped_program, idx, arg) \u001b[38;5;28;01mfor\u001b[39;00m idx, arg \u001b[38;5;129;01min\u001b[39;00m devset}\n\u001b[1;32m     84\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(devset), dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m display_progress)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[1;32m     87\u001b[0m     example_idx, example, prediction, score \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m     88\u001b[0m     reordered_devset\u001b[38;5;241m.\u001b[39mappend((example_idx, example, prediction, score))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n\u001b[1;32m    248\u001b[0m     finished \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39mfinished_futures\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, LabeledFewShot, BootstrapFewShot\n",
    "import dspy.evaluate\n",
    "\n",
    "# make logger appear on jupyter notebook\n",
    "dspy.logger.info = print\n",
    "\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
    "    return answer_EM and answer_PM\n",
    "\n",
    "# Set up a basic optimizer, which will compile our RAG program.\n",
    "optimizer = BootstrapFewShotWithRandomSearch(metric=validate_context_and_answer, max_rounds=1)\n",
    "# optimizer = BootstrapFewShot(metric=validate_context_and_answer, max_bootstrapped_demos=10, max_labeled_demos=10, max_rounds=3)\n",
    "# optimizer.num_candidate_sets = 0\n",
    "\n",
    "langwatch.dspy.init(experiment=\"dspy-visualizer-example\", optimizer=optimizer)\n",
    "\n",
    "# Compile!\n",
    "compiled_rag = optimizer.compile(RAG(), trainset=trainset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
