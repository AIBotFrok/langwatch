{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangWatch DSPy Visualizer\n",
        "\n",
        "This notebook shows an example of a simple DSPy optimization process integrated with LangWatch for training visualization and debugging.\n",
        "\n",
        "[<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/github/langwatch/langwatch/blob/main/python-sdk/examples/dspy_visualization.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgy1Fjhh_lOB"
      },
      "outputs": [],
      "source": [
        "# Install langwatch along with dspy for the visualization\n",
        "!pip install dspy-ai langwatch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51OWavv1CCVV"
      },
      "source": [
        "## Preparing the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xycw8IWs_qnt",
        "outputId": "40844780-608a-4162-cac7-35f57c5764f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OPENAI_API_KEY: ··········\n",
            "LLM test response: ['Hello! How can I assist you today?']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OPENAI_API_KEY: \")\n",
        "\n",
        "import dspy\n",
        "import openai\n",
        "\n",
        "llm = dspy.OpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    max_tokens=2048,\n",
        "    temperature=0,\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        ")\n",
        "\n",
        "print(\"LLM test response:\", llm(\"hello there\"))\n",
        "\n",
        "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
        "dspy.settings.configure(lm=llm, rm=colbertv2_wiki17_abstracts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIAYLNlcCFdO"
      },
      "source": [
        "## Preparing the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXH8qF-QBcEJ",
        "outputId": "203d4516-b9a2-4748-8fcc-7dd64be1342d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(32, 50)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dspy.datasets import HotPotQA\n",
        "\n",
        "# Load the dataset.\n",
        "dataset = HotPotQA(train_seed=1, train_size=32, eval_seed=2023, dev_size=50, test_size=0)\n",
        "\n",
        "# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\n",
        "trainset = [x.with_inputs('question') for x in dataset.train]\n",
        "devset = [x.with_inputs('question') for x in dataset.dev]\n",
        "\n",
        "len(trainset), len(devset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOXtqnmfCNzS"
      },
      "source": [
        "## Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxAaf1IABgxM",
        "outputId": "03155b63-bbaf-4ceb-bf26-07cda4f52b6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Devset] Question: What is the nationality of the chef and restaurateur featured in Restaurant: Impossible?\n",
            "[Devset] Answer: English\n",
            "[Devset] Relevant Wikipedia Titles: {'Restaurant: Impossible', 'Robert Irvine'}\n",
            "[Prediction] Question: What is the nationality of the chef and restaurateur featured in Restaurant: Impossible?\n",
            "[Prediction] Predicted Answer: American\n"
          ]
        }
      ],
      "source": [
        "class GenerateAnswer(dspy.Signature):\n",
        "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
        "\n",
        "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
        "    question = dspy.InputField()\n",
        "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
        "\n",
        "\n",
        "class RAG(dspy.Module):\n",
        "    def __init__(self, num_passages=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
        "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
        "\n",
        "    def forward(self, question):\n",
        "        context = self.retrieve(question).passages\n",
        "        prediction = self.generate_answer(context=context, question=question)\n",
        "        return dspy.Prediction(context=context, answer=prediction.answer)\n",
        "\n",
        "\n",
        "dev_example = devset[18]\n",
        "print(f\"[Devset] Question: {dev_example.question}\")\n",
        "print(f\"[Devset] Answer: {dev_example.answer}\")\n",
        "print(f\"[Devset] Relevant Wikipedia Titles: {dev_example.gold_titles}\")\n",
        "\n",
        "generate_answer = RAG()\n",
        "\n",
        "pred = generate_answer(question=dev_example.question)\n",
        "\n",
        "# Print the input and the prediction.\n",
        "print(f\"[Prediction] Question: {dev_example.question}\")\n",
        "print(f\"[Prediction] Predicted Answer: {pred.answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytbRU9jJCSj8"
      },
      "source": [
        "## Login to LangWatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF9DxTGeCU15",
        "outputId": "29ebbacf-9ca8-4322-fd77-32e47f4c93aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please go to https://app.langwatch.ai/authorize to get your API key\n",
            "Paste your API key here: ··········\n",
            "LangWatch API key set\n"
          ]
        }
      ],
      "source": [
        "import langwatch\n",
        "\n",
        "langwatch.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o69S-BlkE-bV"
      },
      "source": [
        "## Start Training Session!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef67q2B-FCIP",
        "outputId": "cefe1935-1ef3-46ad-a54c-74f08bfb75f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[LangWatch] Experiment initialized, run_id: yellow-mamba-of-proficiency\n",
            "[LangWatch] Open https://app.langwatch.ai/inbox-narrator/experiments/my-awesome-experiment?runIds=yellow-mamba-of-proficiency to track your DSPy training session live\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Average Metric: 10 / 32  (31.2): 100%|██████████| 32/32 [00:00<00:00, 190.05it/s]\n",
            "Average Metric: 11 / 32  (34.4): 100%|██████████| 32/32 [00:00<00:00, 272.69it/s]\n",
            " 34%|███▍      | 11/32 [00:00<00:00, 466.05it/s]\n",
            "Average Metric: 12 / 32  (37.5): 100%|██████████| 32/32 [00:00<00:00, 234.53it/s]\n",
            " 41%|████      | 13/32 [00:00<00:00, 690.03it/s]\n",
            "Average Metric: 12 / 32  (37.5): 100%|██████████| 32/32 [00:00<00:00, 338.70it/s]\n",
            "  9%|▉         | 3/32 [00:00<00:00, 501.19it/s]\n",
            "Average Metric: 10 / 32  (31.2): 100%|██████████| 32/32 [00:00<00:00, 389.88it/s]\n",
            " 12%|█▎        | 4/32 [00:00<00:00, 533.12it/s]\n",
            "Average Metric: 11 / 32  (34.4): 100%|██████████| 32/32 [00:00<00:00, 351.03it/s]\n",
            " 12%|█▎        | 4/32 [00:00<00:00, 478.24it/s]\n",
            "Average Metric: 11 / 32  (34.4): 100%|██████████| 32/32 [00:00<00:00, 616.36it/s]\n",
            " 25%|██▌       | 8/32 [00:00<00:00, 584.59it/s]\n",
            "Average Metric: 10 / 32  (31.2): 100%|██████████| 32/32 [00:00<00:00, 372.51it/s]\n",
            " 19%|█▉        | 6/32 [00:00<00:00, 432.64it/s]\n",
            "Average Metric: 10 / 32  (31.2): 100%|██████████| 32/32 [00:00<00:00, 360.51it/s]\n",
            " 12%|█▎        | 4/32 [00:00<00:00, 447.63it/s]\n",
            "Average Metric: 13 / 32  (40.6): 100%|██████████| 32/32 [00:00<00:00, 283.15it/s]\n",
            " 22%|██▏       | 7/32 [00:00<00:00, 577.93it/s]\n",
            "Average Metric: 10 / 32  (31.2): 100%|██████████| 32/32 [00:00<00:00, 273.38it/s]\n",
            " 28%|██▊       | 9/32 [00:00<00:00, 556.67it/s]\n",
            "Average Metric: 12 / 32  (37.5): 100%|██████████| 32/32 [00:00<00:00, 373.94it/s]\n",
            " 44%|████▍     | 14/32 [00:00<00:00, 692.02it/s]\n",
            "Average Metric: 13 / 32  (40.6): 100%|██████████| 32/32 [00:00<00:00, 324.93it/s]\n",
            "  3%|▎         | 1/32 [00:00<00:00, 526.72it/s]\n",
            "Average Metric: 12 / 32  (37.5): 100%|██████████| 32/32 [00:00<00:00, 586.07it/s]\n",
            " 25%|██▌       | 8/32 [00:00<00:00, 392.20it/s]\n",
            "Average Metric: 12 / 32  (37.5): 100%|██████████| 32/32 [00:00<00:00, 450.29it/s]\n",
            " 28%|██▊       | 9/32 [00:00<00:00, 586.50it/s]\n",
            "Average Metric: 13 / 32  (40.6): 100%|██████████| 32/32 [00:00<00:00, 267.36it/s]\n",
            " 22%|██▏       | 7/32 [00:00<00:00, 503.02it/s]\n",
            "Average Metric: 13 / 32  (40.6): 100%|██████████| 32/32 [00:00<00:00, 358.00it/s]\n",
            "  3%|▎         | 1/32 [00:00<00:00, 383.04it/s]\n",
            "Average Metric: 10 / 32  (31.2): 100%|██████████| 32/32 [00:00<00:00, 382.27it/s]\n",
            " 12%|█▎        | 4/32 [00:00<00:00, 427.18it/s]\n",
            "Average Metric: 11 / 32  (34.4): 100%|██████████| 32/32 [00:00<00:00, 253.22it/s]\n"
          ]
        }
      ],
      "source": [
        "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
        "import dspy.evaluate\n",
        "\n",
        "# Define our metric validation\n",
        "def validate_context_and_answer(example, pred, trace=None):\n",
        "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
        "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
        "    return answer_EM and answer_PM\n",
        "\n",
        "# Set up a basic optimizer, which will compile our RAG program.\n",
        "optimizer = BootstrapFewShotWithRandomSearch(metric=validate_context_and_answer, max_rounds=1, max_bootstrapped_demos=4, max_labeled_demos=4)\n",
        "\n",
        "# Initialize langwatch for this run, to track the optimizer compilation\n",
        "langwatch.dspy.init(experiment=\"my-awesome-experiment\", optimizer=optimizer)\n",
        "\n",
        "# Compile\n",
        "compiled_rag = optimizer.compile(RAG(), trainset=trainset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "u5vA80_JJX-q"
      },
      "outputs": [],
      "source": [
        "compiled_rag.save(\"optimized_model.json\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
